{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12fc83a-bd4b-4157-897d-f6774342f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import vizdoom as vzd\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb647e0-6368-4af8-816b-8ea3caf011d3",
   "metadata": {},
   "source": [
    "# SETUP-GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15493055-d156-464b-a203-8dbf12e75bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DoomGame()\n",
    "game.load_config(r'./scenarios/deadly_corridor-skill-2.cfg')\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47bd952-7230-418f-90eb-ba3a6ea570b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "actions = np.identity(7, dtype=np.uint8)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840aeaf7-c157-4b3e-90e8-770464c9de85",
   "metadata": {},
   "source": [
    "1. actions[0] : MOVE_LEFT\n",
    "2. actions[1] : MOVE_RIGHT\n",
    "3. actions[2] : ATTACK\n",
    "4. actions[3] : MOVE_FORWARD\n",
    "5. actions[4] : MOVE_BACKWARD\n",
    "6. actions[5] : TURN_LEFT\n",
    "7. actions[6] : TURN_RIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd199ad-9ef9-4ad8-996e-4e75ca491654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.new_episode()\n",
    "game.is_episode_finished()\n",
    "game.make_action(random.choice(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2065135d-48a4-46bc-bf99-b2ee30a3ad1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C41290F0>\n",
      "reward: -0.78125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44DD630>\n",
      "reward: -2.458099365234375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8261F0>\n",
      "reward: -1.6581268310546875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C41290F0>\n",
      "reward: -1.1185302734375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44DD630>\n",
      "reward: -7.8141632080078125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8261F0>\n",
      "reward: -2.1456146240234375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA6238B0>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44DD630>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8261F0>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA6238B0>\n",
      "reward: 0.0090179443359375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA827830>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C54F0>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA6238B0>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA827830>\n",
      "reward: 1.03826904296875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C54F0>\n",
      "reward: -1.0473785400390625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8261F0>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C55F0>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C54F0>\n",
      "reward: 1.85662841796875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8261F0>\n",
      "reward: 2.0650482177734375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C55F0>\n",
      "reward: -1.6034698486328125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408D7F0>\n",
      "reward: -2.3198394775390625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8261F0>\n",
      "reward: 6.5553131103515625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C55F0>\n",
      "reward: 10.511260986328125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408D7F0>\n",
      "reward: 7.5960845947265625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5630>\n",
      "reward: 3.8887176513671875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C55F0>\n",
      "reward: -205.32281494140625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408D7F0>\n",
      "////////////////// Result: -192.7489471435547\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5630>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8261F0>\n",
      "reward: 7.110107421875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5AF0>\n",
      "reward: 1.4192962646484375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5630>\n",
      "reward: -2.986175537109375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8261F0>\n",
      "reward: 4.9858245849609375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5AF0>\n",
      "reward: 5.0496826171875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44DD630>\n",
      "reward: 2.28546142578125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8261F0>\n",
      "reward: -5.8689422607421875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5AF0>\n",
      "reward: -0.59515380859375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44DD630>\n",
      "reward: 3.318267822265625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408D7F0>\n",
      "reward: 2.2381591796875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5B30>\n",
      "reward: -5.4407501220703125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44DD630>\n",
      "reward: -7.3199005126953125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408D7F0>\n",
      "reward: 2.0127410888671875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C411DD30>\n",
      "reward: -12.321212768554688\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44F9BB0>\n",
      "reward: -9.81146240234375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408D7F0>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C411DD30>\n",
      "reward: 5.2261199951171875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44F9BB0>\n",
      "reward: 11.923919677734375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408D7F0>\n",
      "reward: 10.796859741210938\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: 7.4561767578125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44F9BB0>\n",
      "reward: 5.2463836669921875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408D7F0>\n",
      "reward: 10.638931274414062\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: 11.318130493164062\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44F9BB0>\n",
      "reward: 4.2124481201171875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5E70>\n",
      "reward: 1.273162841796875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: 0.9898834228515625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44F9BB0>\n",
      "reward: 7.9872589111328125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5E70>\n",
      "reward: 2.0158233642578125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: -2.368896484375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: 5.394500732421875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5E70>\n",
      "reward: -0.7975616455078125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: -5.1263275146484375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: -3.1128692626953125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5AF0>\n",
      "reward: 5.66510009765625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: 6.2430572509765625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: -3.438140869140625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5AF0>\n",
      "reward: -4.7413330078125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44F9DB0>\n",
      "reward: -9.534439086914062\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA826030>\n",
      "reward: -203.61097717285156\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5AF0>\n",
      "////////////////// Result: -152.266845703125\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44F9DB0>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: 7.110260009765625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44DD630>\n",
      "reward: 8.529800415039062\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44F9DB0>\n",
      "reward: -1.2768707275390625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: -4.55322265625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44DD630>\n",
      "reward: 3.95880126953125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: -0.668304443359375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: -11.173049926757812\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44DD630>\n",
      "reward: -11.228408813476562\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: -0.7296142578125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: 4.9585723876953125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C6470>\n",
      "reward: -1.4583892822265625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: -9.35333251953125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: -0.1098785400390625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C6470>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: 1.85662841796875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: 2.22723388671875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA82CBF0>\n",
      "reward: -0.38714599609375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: -0.7768402099609375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: -2.8935089111328125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA82CBF0>\n",
      "reward: -0.0245819091796875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C411F4F0>\n",
      "reward: -0.0027313232421875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C6530>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: 1.59765625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C411F4F0>\n",
      "reward: 1.9361724853515625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C6530>\n",
      "reward: 2.1366119384765625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: 1.8647308349609375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C411E8F0>\n",
      "reward: -7.11376953125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C6530>\n",
      "reward: 4.0269775390625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: 6.704437255859375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C411E8F0>\n",
      "reward: 5.3580322265625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44FA4B0>\n",
      "reward: 3.2169342041015625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C5FF0>\n",
      "reward: 1.7308197021484375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C411E8F0>\n",
      "reward: -0.4843902587890625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44FA4B0>\n",
      "reward: -2.598846435546875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: -2.4945220947265625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C66B0>\n",
      "reward: -0.84686279296875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44FA4B0>\n",
      "reward: -0.132354736328125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: -1.748748779296875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C66B0>\n",
      "reward: -2.051055908203125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44FA4B0>\n",
      "reward: -0.547698974609375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C411E8F0>\n",
      "reward: 0.069427490234375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: 0.882598876953125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44FA4B0>\n",
      "reward: -0.1011962890625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C411E8F0>\n",
      "reward: -0.58294677734375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: -0.3933258056640625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44FA4B0>\n",
      "reward: -3.2983245849609375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: 1.9382781982421875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: -0.640716552734375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44FA4B0>\n",
      "reward: -1.990386962890625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: -6.363250732421875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C66B0>\n",
      "reward: -0.0611419677734375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA715FF0>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C66B0>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C68F0>\n",
      "reward: 5.5503387451171875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C66B0>\n",
      "reward: 1.107818603515625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C408DB30>\n",
      "reward: -6.4349822998046875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C68F0>\n",
      "reward: 3.2325286865234375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C66B0>\n",
      "reward: 10.781356811523438\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA82EB30>\n",
      "reward: 7.5106353759765625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C68F0>\n",
      "reward: -10.135284423828125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C66B0>\n",
      "reward: -11.61419677734375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA82EB30>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C68F0>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C6B70>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA82EB30>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C68F0>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C6B70>\n",
      "reward: 0.0\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA82EB30>\n",
      "reward: 5.3054046630859375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA82E930>\n",
      "reward: 11.669967651367188\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C6B70>\n",
      "reward: 11.853225708007812\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA82EB30>\n",
      "reward: 3.35174560546875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA82E930>\n",
      "reward: 4.4688720703125\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C6B70>\n",
      "reward: 5.364105224609375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C6B30>\n",
      "reward: 3.618072509765625\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA82E930>\n",
      "reward: 7.745819091796875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C6B70>\n",
      "reward: 3.5358734130859375\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA8C6B30>\n",
      "reward: 0.0272216796875\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158DA82E930>\n",
      "reward: -201.1285858154297\n",
      "ammo None\n",
      "state <vizdoom.vizdoom.GameState object at 0x00000158C44FBAF0>\n",
      "////////////////// Result: -160.1715087890625\n"
     ]
    }
   ],
   "source": [
    "episodes = 3\n",
    "for e in range(episodes):\n",
    "    game.new_episode()\n",
    "    while not game.is_episode_finished():\n",
    "        satate=game.get_state()\n",
    "        state = game.get_state()\n",
    "        img = state.screen_buffer\n",
    "        # Get the game variables - ammo\n",
    "        info = state.game_variables\n",
    "        reward = game.make_action(random.choice(actions),4) # frame skip=4 time for agent to process\n",
    "        print('reward:', reward) \n",
    "        print(\"ammo\",info)\n",
    "        print(\"state\",state)\n",
    "        time.sleep(0.02)\n",
    "    print('////////////////// Result:', game.get_total_reward())\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f85591b-2316-427f-abf2-547235c29511",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae116f9-e4f4-4b30-9cbb-19380653cd31",
   "metadata": {},
   "source": [
    "## Converting it to a Gymnasium Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1caf887e-2ffa-4a19-b0db-3e1819022ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "import torchaudio\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d09d280e-917b-4046-a12f-8c66c105997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym(Env):\n",
    "    def __init__(self, render=False,config='./scenarios/deadly_corridor-skill-2.cfg'):\n",
    "        super().__init__()\n",
    "        self.game = vzd.DoomGame()\n",
    "        self.game.load_config(config)\n",
    "\n",
    "        # Render frame logic\n",
    "        if not render:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        self.game.init()\n",
    "\n",
    "        # Create the action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(7)  # 7 possible actions\n",
    "        self.actions=np.identity(7, dtype=np.float32)\n",
    "        \n",
    "\n",
    "\n",
    "    def custom_reward(self, prev_state, current_state):\n",
    "        reward = 0\n",
    "    \n",
    "        # Extract game variables\n",
    "        prev_health = prev_state.game_variables[0]  # HEALTH\n",
    "        prev_hits = prev_state.game_variables[1]  # HITCOUNT\n",
    "        prev_ammo = prev_state.game_variables[2]  # SELECTED_WEAPON_AMMO\n",
    "        prev_kills = prev_state.game_variables[3]  # KILLCOUNT\n",
    "        prev_dmg = prev_state.game_variables[4]  # KILLCOUNT\n",
    "        prev_dmg_deal = prev_state.game_variables[5]  # KILLCOUNT\n",
    "        \n",
    "        current_health = current_state.game_variables[0]  # HEALTH\n",
    "        current_hits = current_state.game_variables[1]  # HITCOUNT\n",
    "        current_ammo = current_state.game_variables[2]  # SELECTED_WEAPON_AMMO\n",
    "        current_kills = current_state.game_variables[3]  # KILLCOUNT\n",
    "        current_dmg = current_state.game_variables[4]  # KILLCOUNT\n",
    "        current_dmg_deal = current_state.game_variables[5]  # KILLCOUNT\n",
    "        \n",
    "        ammo_delta=current_ammo-prev_ammo \n",
    "        hitcount_delta= current_dmg_deal - prev_dmg_deal\n",
    "        damage_taken_delta=-current_dmg+prev_dmg\n",
    "        \n",
    "        reward = damage_taken_delta*50 + hitcount_delta*200  + ammo_delta*40 \n",
    "        \n",
    "    \n",
    "\n",
    "        return reward\n",
    "        \n",
    "    def step(self, action):\n",
    "        prev_state = self.game.get_state()  # Store the previous state\n",
    "        reward = self.game.make_action(self.actions[action], 4)  # Default reward\n",
    "        current_state = self.game.get_state()  # Get the current state\n",
    "\n",
    "        # Compute custom reward\n",
    "        if prev_state is not None and current_state is not None:\n",
    "            reward += self.custom_reward(prev_state, current_state)\n",
    "\n",
    "        terminated = self.game.is_episode_finished()\n",
    "        truncated = self.game.get_episode_time() >= self.game.get_episode_timeout()\n",
    "\n",
    "        state = np.zeros(self.observation_space.shape, dtype=np.uint8)  # Default blank state\n",
    "        info = {\"ammo\": 0}  # Default info\n",
    "\n",
    "        if not (terminated or truncated):\n",
    "            game_state = self.game.get_state()\n",
    "            if game_state is not None:\n",
    "                state = self.grayscale(game_state.screen_buffer)\n",
    "                info = {\"ammo\": game_state.game_variables[0]}\n",
    "\n",
    "        return state, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Restart the game and return the initial state.\"\"\"\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"Convert the observation to grayscale and resize it.\"\"\"\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100, 160, 1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the game.\"\"\"\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236d716c-593f-4734-a553-cdc2ada4930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e43627cd-2e12-41f7-a9fc-f7f275a5fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_Deadly_Corridor_COMP_5_S_2'\n",
    "LOG_DIR = './logs/log_Deadly_Corridor_s2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab49e155-c2bf-4413-a885-210d9e01ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb6db91-fe8e-4e62-bec1-1eb3677dc435",
   "metadata": {},
   "source": [
    "### Testing the model \n",
    "`./train/train_Deadly_Corridor_COMP_4/best_model_80000.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d95c46d6-1407-4b3e-8328-4cd59169cc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 4999.07600402832\n",
      "Episode 2: Total Reward = 16656.90953063965\n",
      "Episode 3: Total Reward = 14519.84310913086\n",
      "Episode 4: Total Reward = 3979.9686431884766\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN, PPO \n",
    "import cv2\n",
    "\n",
    "\n",
    "model_path = \"./train/train_Deadly_Corridor_COMP_4/best_model_80000.zip\"  \n",
    "model = PPO.load(model_path)\n",
    "\n",
    "\n",
    "env = VizDoomGym(render=True)  \n",
    "num_episodes = 4\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs,_ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        time.sleep(0.10)\n",
    "        total_reward += reward\n",
    "        done=terminated or truncated\n",
    "        # time.sleep(1)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "    time.sleep(2)\n",
    "  \n",
    "# Close environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13699a9b-3a24-482a-8978-d6fefcfd21b5",
   "metadata": {},
   "source": [
    "**The agent is doing surprisingly well -- but it need one more session of training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3ef85ed-06da-4a1d-be06-2e74438a4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_Deadly_Corridor'\n",
    "LOG_DIR = './logs/log_Deadly_Corridor'\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)\n",
    "env=VizDoomGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d739681e-fdd4-4747-8356-9e33242c96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_path = \"./train/train_Deadly_Corridor_COMP_4/best_model_80000.zip\"  \n",
    "model = PPO.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "510260a7-bd77-40c1-94d2-d88dcd8df3be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_Deadly_Corridor\\PPO_13\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 205      |\n",
      "|    ep_rew_mean     | 6.95e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 12       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 654      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 186        |\n",
      "|    ep_rew_mean          | 6.65e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 12         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 1328       |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01837518 |\n",
      "|    clip_fraction        | 0.478      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -1.35      |\n",
      "|    explained_variance   | 0.423      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 1.71e+05   |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | 0.0175     |\n",
      "|    value_loss           | 3.28e+05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 180         |\n",
      "|    ep_rew_mean          | 6.57e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1996        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011619838 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.44        |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.12e+05    |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    value_loss           | 3.56e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 212         |\n",
      "|    ep_rew_mean          | 7.46e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2673        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017511304 |\n",
      "|    clip_fraction        | 0.455       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.23e+05    |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | 0.0162      |\n",
      "|    value_loss           | 3.4e+05     |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 221      |\n",
      "|    ep_rew_mean          | 7.92e+03 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 12       |\n",
      "|    iterations           | 5        |\n",
      "|    time_elapsed         | 3313     |\n",
      "|    total_timesteps      | 40960    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.024725 |\n",
      "|    clip_fraction        | 0.477    |\n",
      "|    clip_range           | 0.1      |\n",
      "|    entropy_loss         | -1.29    |\n",
      "|    explained_variance   | 0.426    |\n",
      "|    learning_rate        | 1e-05    |\n",
      "|    loss                 | 2.19e+05 |\n",
      "|    n_updates            | 590      |\n",
      "|    policy_gradient_loss | 0.0137   |\n",
      "|    value_loss           | 2.94e+05 |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 207         |\n",
      "|    ep_rew_mean          | 7.43e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 4007        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011969889 |\n",
      "|    clip_fraction        | 0.434       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 5.28e+04    |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | 0.00899     |\n",
      "|    value_loss           | 3.16e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 186         |\n",
      "|    ep_rew_mean          | 7.48e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 4710        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010876583 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 3.73e+05    |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | 0.00797     |\n",
      "|    value_loss           | 3.58e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 156         |\n",
      "|    ep_rew_mean          | 7.8e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 6406        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013219323 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.81e+05    |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | 0.00997     |\n",
      "|    value_loss           | 3.65e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 142          |\n",
      "|    ep_rew_mean          | 8.27e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 7108         |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102635585 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.61e+05     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | 0.00983      |\n",
      "|    value_loss           | 4.48e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 132         |\n",
      "|    ep_rew_mean          | 7.82e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 7768        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010434208 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.81e+05    |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    value_loss           | 4.53e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 122         |\n",
      "|    ep_rew_mean          | 7.58e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 8418        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009570441 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.992      |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.5e+05     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | 0.00838     |\n",
      "|    value_loss           | 4.69e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 129         |\n",
      "|    ep_rew_mean          | 8.75e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 9076        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008791216 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.948      |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.26e+05    |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | 0.0092      |\n",
      "|    value_loss           | 5.11e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 136         |\n",
      "|    ep_rew_mean          | 9e+03       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 9769        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010501081 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.979      |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.15e+05    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | 0.0109      |\n",
      "|    value_loss           | 5.07e+05    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.set_env(env)\n",
    "model.learn(total_timesteps=100000, callback=callback)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551be19-8877-4b1d-89ff-7ae190673f29",
   "metadata": {},
   "source": [
    "**Testing the model `./train/train_Deadly_Corridor_COMP_4/best_model_90000.zip`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac03402f-e229-4153-9eb6-e5b61213fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 10700.239730834961\n",
      "Episode 2: Total Reward = 11720.593460083008\n",
      "Episode 3: Total Reward = 9207.931289672852\n",
      "Episode 4: Total Reward = 14296.758163452148\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN, PPO \n",
    "import cv2\n",
    "\n",
    "\n",
    "model_path = \"./train/train_Deadly_Corridor/best_model_100000.zip\"  \n",
    "model = PPO.load(model_path)\n",
    "\n",
    "\n",
    "env = VizDoomGym(render=True)  \n",
    "num_episodes = 4\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs,_ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        time.sleep(0.10)\n",
    "        total_reward += reward\n",
    "        done=terminated or truncated\n",
    "        # time.sleep(1)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "    time.sleep(2)\n",
    "  \n",
    "# Close environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da06c8a-b3d4-4c4e-84b7-d8e657189d27",
   "metadata": {},
   "source": [
    "**The agent is one shotting all the enemy sometimes he skip one enemy it's will be trained with the next level of difficulty**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
