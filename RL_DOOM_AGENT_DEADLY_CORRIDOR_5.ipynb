{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12fc83a-bd4b-4157-897d-f6774342f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import vizdoom as vzd\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb647e0-6368-4af8-816b-8ea3caf011d3",
   "metadata": {},
   "source": [
    "# SETUP-GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15493055-d156-464b-a203-8dbf12e75bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DoomGame()\n",
    "game.load_config(r'./scenarios/deadly_corridor-skill-5.cfg')\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47bd952-7230-418f-90eb-ba3a6ea570b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "actions = np.identity(7, dtype=np.uint8)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840aeaf7-c157-4b3e-90e8-770464c9de85",
   "metadata": {},
   "source": [
    "1. actions[0] : MOVE_LEFT\n",
    "2. actions[1] : MOVE_RIGHT\n",
    "3. actions[2] : ATTACK\n",
    "4. actions[3] : MOVE_FORWARD\n",
    "5. actions[4] : MOVE_BACKWARD\n",
    "6. actions[5] : TURN_LEFT\n",
    "7. actions[6] : TURN_RIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd199ad-9ef9-4ad8-996e-4e75ca491654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.new_episode()\n",
    "game.is_episode_finished()\n",
    "game.make_action(random.choice(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2065135d-48a4-46bc-bf99-b2ee30a3ad1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n",
      "ammo [100.   0.  -1.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535BBE770>\n",
      "reward: -0.78125\n",
      "ammo [100.   0.  52.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BE570>\n",
      "reward: -12.2364501953125\n",
      "ammo [100.   0.  52.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F86D4B0>\n",
      "reward: -2.854705810546875\n",
      "ammo [88.  0. 52.  0. 12.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F874770>\n",
      "reward: -0.0828399658203125\n",
      "ammo [88.  0. 52.  0. 12.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D5359E5AB0>\n",
      "reward: 0.0\n",
      "ammo [76.  0. 52.  0. 24.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D5359E7730>\n",
      "reward: -0.0218505859375\n",
      "ammo [76.  0. 52.  0. 24.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F874770>\n",
      "reward: 6.7374725341796875\n",
      "ammo [76.  0. 52.  0. 24.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D5359E5AB0>\n",
      "reward: 8.192733764648438\n",
      "ammo [76.  0. 52.  0. 24.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D5359E7730>\n",
      "reward: 5.526031494140625\n",
      "ammo [76.  0. 52.  0. 24.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F41CAF0>\n",
      "reward: 0.392059326171875\n",
      "ammo [76.  0. 52.  0. 24.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BCB70>\n",
      "reward: -9.11175537109375\n",
      "ammo [46.  0. 52.  0. 54.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D5359E7730>\n",
      "reward: -11.67669677734375\n",
      "ammo [22.  0. 52.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F41CAF0>\n",
      "reward: -0.0590667724609375\n",
      "ammo [22.  0. 52.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BCB70>\n",
      "reward: 0.0\n",
      "ammo [22.  1. 51.  1. 78.  5.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D5359E7730>\n",
      "reward: 0.0\n",
      "ammo [22.  1. 51.  1. 78.  5.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D5359E5AB0>\n",
      "reward: -0.0152740478515625\n",
      "ammo [22.  1. 51.  1. 78.  5.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BCB70>\n",
      "reward: 0.0\n",
      "ammo [22.  1. 51.  1. 78.  5.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D5359E7730>\n",
      "reward: -200.0\n",
      "ammo [22.  1. 51.  1. 78.  5.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D5359E5AB0>\n",
      "////////////////// Result: -215.99159240722656\n",
      "reward: 0.0\n",
      "ammo [100.   0.  -1.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BCB70>\n",
      "reward: 0.0\n",
      "ammo [100.   0.  52.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535C5A9F0>\n",
      "reward: 0.0\n",
      "ammo [100.   0.  52.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535C5AAF0>\n",
      "reward: 1.2447357177734375\n",
      "ammo [100.   0.  52.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BCB70>\n",
      "reward: 1.493194580078125\n",
      "ammo [100.   0.  52.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F87EB70>\n",
      "reward: 0.281005859375\n",
      "ammo [100.   0.  52.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D5359E5AB0>\n",
      "reward: 0.0\n",
      "ammo [100.   0.  51.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BCB70>\n",
      "reward: -5.4846343994140625\n",
      "ammo [100.   0.  51.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F87EB70>\n",
      "reward: -6.131591796875\n",
      "ammo [100.   0.  51.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535BB2BF0>\n",
      "reward: 3.2933807373046875\n",
      "ammo [100.   0.  51.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BCB70>\n",
      "reward: 5.9411773681640625\n",
      "ammo [100.   0.  51.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F87EB70>\n",
      "reward: -15.099594116210938\n",
      "ammo [100.   0.  51.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535BB2C30>\n",
      "reward: -0.41644287109375\n",
      "ammo [22.  0. 51.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BCB70>\n",
      "reward: -1.097381591796875\n",
      "ammo [22.  0. 51.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BEB70>\n",
      "reward: 0.0\n",
      "ammo [22.  0. 51.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535BB2C30>\n",
      "reward: 0.0\n",
      "ammo [22.  0. 51.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535BB2C70>\n",
      "reward: 0.0\n",
      "ammo [22.  0. 51.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BEB70>\n",
      "reward: 0.0\n",
      "ammo [22.  0. 51.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535BB2C30>\n",
      "reward: 0.0\n",
      "ammo [22.  0. 51.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535C2F4B0>\n",
      "reward: -0.0115814208984375\n",
      "ammo [22.  0. 51.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BEB70>\n",
      "reward: 0.0\n",
      "ammo [22.  0. 50.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535B3AF30>\n",
      "reward: 0.0\n",
      "ammo [22.  0. 50.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535C2F4B0>\n",
      "reward: 0.0\n",
      "ammo [22.  0. 50.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BEB70>\n",
      "reward: 5.5538330078125\n",
      "ammo [22.  0. 49.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BCB70>\n",
      "reward: 2.237060546875\n",
      "ammo [22.  0. 49.  0. 78.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535C2F4B0>\n",
      "reward: -5.970947265625\n",
      "ammo [10.  0. 49.  0. 90.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BEB70>\n",
      "reward: 1.14959716796875\n",
      "ammo [10.  0. 49.  0. 90.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535BB2C30>\n",
      "reward: -197.2843780517578\n",
      "ammo [10.  0. 48.  0. 90.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535C2F4B0>\n",
      "////////////////// Result: -210.3025665283203\n",
      "reward: 0.0\n",
      "ammo [100.   0.  -1.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535B3AF30>\n",
      "reward: 0.78125\n",
      "ammo [100.   0.  52.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BCB70>\n",
      "reward: 9.571823120117188\n",
      "ammo [100.   0.  52.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535C2F4B0>\n",
      "reward: 10.191940307617188\n",
      "ammo [100.   0.  52.   0.   0.   0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535B3AF30>\n",
      "reward: -24.333984375\n",
      "ammo [40.  0. 52.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535BB2C30>\n",
      "reward: -12.062103271484375\n",
      "ammo [40.  0. 52.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535C2F4B0>\n",
      "reward: 0.0\n",
      "ammo [40.  0. 52.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BCB70>\n",
      "reward: 0.0\n",
      "ammo [40.  0. 52.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BF0B0>\n",
      "reward: 0.0\n",
      "ammo [40.  0. 52.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535C2F4B0>\n",
      "reward: 0.0\n",
      "ammo [40.  0. 52.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BCB70>\n",
      "reward: 0.0\n",
      "ammo [40.  0. 52.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535BB2C30>\n",
      "reward: 0.0\n",
      "ammo [40.  0. 51.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535C2F4B0>\n",
      "reward: 0.0\n",
      "ammo [40.  0. 51.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BEB70>\n",
      "reward: -0.1285552978515625\n",
      "ammo [40.  0. 51.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535BB2C30>\n",
      "reward: 0.0\n",
      "ammo [40.  0. 51.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535C2F4B0>\n",
      "reward: 0.0\n",
      "ammo [40.  0. 51.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BEB70>\n",
      "reward: 0.0\n",
      "ammo [40.  0. 50.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535BB2C30>\n",
      "reward: 0.0\n",
      "ammo [40.  0. 50.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535A3DEB0>\n",
      "reward: 5.9388427734375\n",
      "ammo [40.  0. 50.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D51F4BEB70>\n",
      "reward: 3.3697509765625\n",
      "ammo [40.  0. 49.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535BB2C30>\n",
      "reward: -197.9391632080078\n",
      "ammo [40.  0. 49.  0. 60.  0.]\n",
      "state <vizdoom.vizdoom.GameState object at 0x000001D535A3DEB0>\n",
      "////////////////// Result: -204.61019897460938\n"
     ]
    }
   ],
   "source": [
    "episodes = 3\n",
    "for e in range(episodes):\n",
    "    game.new_episode()\n",
    "    while not game.is_episode_finished():\n",
    "        satate=game.get_state()\n",
    "        state = game.get_state()\n",
    "        img = state.screen_buffer\n",
    "        # Get the game variables - ammo\n",
    "        info = state.game_variables\n",
    "        reward = game.make_action(random.choice(actions),4) # frame skip=4 time for agent to process\n",
    "        print('reward:', reward) \n",
    "        print(\"ammo\",info)\n",
    "        print(\"state\",state)\n",
    "        time.sleep(0.02)\n",
    "    print('////////////////// Result:', game.get_total_reward())\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f85591b-2316-427f-abf2-547235c29511",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1caf887e-2ffa-4a19-b0db-3e1819022ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "import torchaudio\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09d280e-917b-4046-a12f-8c66c105997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym(Env):\n",
    "    def __init__(self, render=False,config='./scenarios/deadly_corridor-skill-5.cfg'):\n",
    "        super().__init__()\n",
    "        self.game = vzd.DoomGame()\n",
    "        self.game.load_config(config)\n",
    "\n",
    "        # Render frame logic\n",
    "        if not render:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        self.game.init()\n",
    "\n",
    "        # Create the action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(7)  # 7 possible actions\n",
    "        self.actions=np.identity(7, dtype=np.float32)\n",
    "        \n",
    "\n",
    "\n",
    "    def custom_reward(self, prev_state, current_state):\n",
    "        reward = 0\n",
    "    \n",
    "        # Extract game variables\n",
    "        prev_health = prev_state.game_variables[0]  # HEALTH\n",
    "        prev_hits = prev_state.game_variables[1]  # HITCOUNT\n",
    "        prev_ammo = prev_state.game_variables[2]  # SELECTED_WEAPON_AMMO\n",
    "        prev_kills = prev_state.game_variables[3]  # KILLCOUNT\n",
    "        prev_dmg = prev_state.game_variables[4]  # KILLCOUNT\n",
    "        prev_dmg_deal = prev_state.game_variables[5]  # KILLCOUNT\n",
    "        \n",
    "        current_health = current_state.game_variables[0]  # HEALTH\n",
    "        current_hits = current_state.game_variables[1]  # HITCOUNT\n",
    "        current_ammo = current_state.game_variables[2]  # SELECTED_WEAPON_AMMO\n",
    "        current_kills = current_state.game_variables[3]  # KILLCOUNT\n",
    "        current_dmg = current_state.game_variables[4]  # KILLCOUNT\n",
    "        current_dmg_deal = current_state.game_variables[5]  # KILLCOUNT\n",
    "        \n",
    "        ammo_delta=current_ammo-prev_ammo \n",
    "        hitcount_delta= current_dmg_deal - prev_dmg_deal\n",
    "        damage_taken_delta=-current_dmg+prev_dmg\n",
    "        \n",
    "        reward = damage_taken_delta*60 + hitcount_delta*200  + ammo_delta*50 \n",
    "        \n",
    "    \n",
    "\n",
    "        return reward\n",
    "        \n",
    "    def step(self, action):\n",
    "        prev_state = self.game.get_state()  # Store the previous state\n",
    "        reward = self.game.make_action(self.actions[action], 4)  # Default reward\n",
    "        current_state = self.game.get_state()  # Get the current state\n",
    "\n",
    "        # Compute custom reward\n",
    "        if prev_state is not None and current_state is not None:\n",
    "            reward += self.custom_reward(prev_state, current_state)\n",
    "\n",
    "        terminated = self.game.is_episode_finished()\n",
    "        truncated = self.game.get_episode_time() >= self.game.get_episode_timeout()\n",
    "\n",
    "        state = np.zeros(self.observation_space.shape, dtype=np.uint8)  # Default blank state\n",
    "        info = {\"ammo\": 0}  # Default info\n",
    "\n",
    "        if not (terminated or truncated):\n",
    "            game_state = self.game.get_state()\n",
    "            if game_state is not None:\n",
    "                state = self.grayscale(game_state.screen_buffer)\n",
    "                info = {\"ammo\": game_state.game_variables[0]}\n",
    "\n",
    "        return state, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Restart the game and return the initial state.\"\"\"\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"Convert the observation to grayscale and resize it.\"\"\"\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100, 160, 1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the game.\"\"\"\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236d716c-593f-4734-a553-cdc2ada4930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb6db91-fe8e-4e62-bec1-1eb3677dc435",
   "metadata": {},
   "source": [
    "### Testing the model \n",
    "`./train/train_Deadly_Corridor_COMP_5_S_4/best_model_100000.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d95c46d6-1407-4b3e-8328-4cd59169cc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 3773.2894439697266\n",
      "Episode 2: Total Reward = 9053.726379394531\n",
      "Episode 3: Total Reward = 13268.862609863281\n",
      "Episode 4: Total Reward = 10170.678039550781\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN, PPO \n",
    "import cv2\n",
    "\n",
    "\n",
    "model_path = \"./train/train_Deadly_Corridor_COMP_5_S_4/best_model_100000.zip\"  \n",
    "model = PPO.load(model_path)\n",
    "\n",
    "\n",
    "env = VizDoomGym(render=True)  \n",
    "num_episodes = 4\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs,_ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        time.sleep(0.10)\n",
    "        total_reward += reward\n",
    "        done=terminated or truncated\n",
    "        # time.sleep(1)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "    time.sleep(2)\n",
    "  \n",
    "# Close environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13699a9b-3a24-482a-8978-d6fefcfd21b5",
   "metadata": {},
   "source": [
    "**The agent is doing surprisingly well -- but it need more training** \n",
    "<br>\n",
    "*new reward:*<br>\n",
    "`damage_taken_delta*55 + hitcount_delta*200  + ammo_delta*45 ` <br>\n",
    "*old reward:*<br>\n",
    "`damage_taken_delta*60 + hitcount_delta*200  + ammo_delta*50 `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e43627cd-2e12-41f7-a9fc-f7f275a5fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_Deadly_Corridor_COMP_5_S_5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3ef85ed-06da-4a1d-be06-2e74438a4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)\n",
    "env=VizDoomGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d739681e-fdd4-4747-8356-9e33242c96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_path = \"./train/train_Deadly_Corridor_COMP_5_S_4/best_model_100000.zip\"  \n",
    "model = PPO.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "510260a7-bd77-40c1-94d2-d88dcd8df3be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_Deadly_Corridor\\PPO_17\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.3     |\n",
      "|    ep_rew_mean     | 7.54e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 9        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 824      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 25.8      |\n",
      "|    ep_rew_mean          | -769      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 10        |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 1579      |\n",
      "|    total_timesteps      | 16384     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4681243 |\n",
      "|    clip_fraction        | 0.538     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | -0.818    |\n",
      "|    explained_variance   | 0.0223    |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | 2.2e+05   |\n",
      "|    n_updates            | 860       |\n",
      "|    policy_gradient_loss | 0.14      |\n",
      "|    value_loss           | 6.08e+05  |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 26.2        |\n",
      "|    ep_rew_mean          | -1.09e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 2269        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.099951975 |\n",
      "|    clip_fraction        | 0.568       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.874      |\n",
      "|    explained_variance   | -0.243      |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 6.09e+05    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | 0.047       |\n",
      "|    value_loss           | 1.11e+06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 24.9        |\n",
      "|    ep_rew_mean          | -1.04e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2995        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017722119 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.813      |\n",
      "|    explained_variance   | 0.289       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 4.91e+05    |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | 0.0122      |\n",
      "|    value_loss           | 9.57e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 23.3       |\n",
      "|    ep_rew_mean          | -968       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 10         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 3728       |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02619851 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -0.805     |\n",
      "|    explained_variance   | 0.378      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 3.33e+05   |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | 0.00817    |\n",
      "|    value_loss           | 9.03e+05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37.8        |\n",
      "|    ep_rew_mean          | 284         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 4468        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012782825 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.725      |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 4.11e+05    |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | 0.00583     |\n",
      "|    value_loss           | 9.06e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 38.1        |\n",
      "|    ep_rew_mean          | 900         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 5268        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019347368 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.759      |\n",
      "|    explained_variance   | 0.423       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.36e+05    |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | 0.00665     |\n",
      "|    value_loss           | 8.1e+05     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 43.7        |\n",
      "|    ep_rew_mean          | 1.05e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 5918        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020646676 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.725      |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 3.4e+05     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | 0.0094      |\n",
      "|    value_loss           | 8.07e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 52.7        |\n",
      "|    ep_rew_mean          | 1.54e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 6561        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018950589 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.698      |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.29e+05    |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | 0.0104      |\n",
      "|    value_loss           | 7.51e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 63.3        |\n",
      "|    ep_rew_mean          | 3.06e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 7205        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024147443 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.75e+05    |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | 0.0126      |\n",
      "|    value_loss           | 7e+05       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 67.7        |\n",
      "|    ep_rew_mean          | 3.99e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 7845        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019752048 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.696      |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.48e+05    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 0.0136      |\n",
      "|    value_loss           | 5.59e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 83         |\n",
      "|    ep_rew_mean          | 4.81e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 11         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 8482       |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01873159 |\n",
      "|    clip_fraction        | 0.343      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -0.64      |\n",
      "|    explained_variance   | 0.472      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 4.31e+05   |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | 0.0165     |\n",
      "|    value_loss           | 5.59e+05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 76.5        |\n",
      "|    ep_rew_mean          | 4.49e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 9118        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021467824 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.65       |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.33e+05    |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | 0.0176      |\n",
      "|    value_loss           | 5.29e+05    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.set_env(env)\n",
    "model.learn(total_timesteps=100000, callback=callback)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551be19-8877-4b1d-89ff-7ae190673f29",
   "metadata": {},
   "source": [
    "##### **Testing the model `./train/train_Deadly_Corridor_COMP_5_S_5/best_model_100000.zip`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac03402f-e229-4153-9eb6-e5b61213fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 2931.184600830078\n",
      "Episode 2: Total Reward = 2513.8346405029297\n",
      "Episode 3: Total Reward = 6499.580520629883\n",
      "Episode 4: Total Reward = 7243.383407592773\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN, PPO \n",
    "import cv2\n",
    "\n",
    "\n",
    "model_path = \"./train/train_Deadly_Corridor_COMP_5_S_5/best_model_100000.zip\"  \n",
    "model = PPO.load(model_path)\n",
    "\n",
    "\n",
    "env = VizDoomGym(render=True)  \n",
    "num_episodes = 4\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs,_ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        time.sleep(0.10)\n",
    "        total_reward += reward\n",
    "        done=terminated or truncated\n",
    "        # time.sleep(1)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "    time.sleep(2)\n",
    "  \n",
    "# Close environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c111ee-9cbd-4213-88e4-b1664f469d2c",
   "metadata": {},
   "source": [
    "### The agent need more training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12aa57e5-a3e4-4754-9396-936d7e98b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_Deadly_Corridor_COMP_5_S_5_2'\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)\n",
    "env=VizDoomGym()\n",
    "model = model_path = \"./train/train_Deadly_Corridor_COMP_5_S_5/best_model_100000.zip\"  \n",
    "model = PPO.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aace58fe-4500-4d1c-a6e5-15082bf9477b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_Deadly_Corridor\\PPO_18\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81.1     |\n",
      "|    ep_rew_mean     | 4.75e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 12       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 648      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 82.1       |\n",
      "|    ep_rew_mean          | 4.91e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 11         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 1376       |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02385709 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -0.62      |\n",
      "|    explained_variance   | 0.515      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 3.35e+05   |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | 0.0151     |\n",
      "|    value_loss           | 5.15e+05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 77.6        |\n",
      "|    ep_rew_mean          | 5.04e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 2116        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025447812 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 0.492       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 3.14e+05    |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | 0.0189      |\n",
      "|    value_loss           | 5.55e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 89.5        |\n",
      "|    ep_rew_mean          | 6.33e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2867        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020618021 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.594      |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.78e+05    |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | 0.0165      |\n",
      "|    value_loss           | 5.28e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 82.8        |\n",
      "|    ep_rew_mean          | 5.76e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 3581        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025788987 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.532      |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.86e+05    |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | 0.0199      |\n",
      "|    value_loss           | 5.2e+05     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 89.9        |\n",
      "|    ep_rew_mean          | 6.29e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 4234        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015558209 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.84e+05    |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | 0.0133      |\n",
      "|    value_loss           | 5.36e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 81.7        |\n",
      "|    ep_rew_mean          | 5.75e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 4884        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017218895 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | 0.504       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.57e+05    |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | 0.0157      |\n",
      "|    value_loss           | 5.02e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 88.1        |\n",
      "|    ep_rew_mean          | 7.17e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 5545        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017761879 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.475      |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.28e+05    |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | 0.0148      |\n",
      "|    value_loss           | 5.49e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 91.2       |\n",
      "|    ep_rew_mean          | 7.18e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 11         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 6276       |\n",
      "|    total_timesteps      | 73728      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01940979 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -0.473     |\n",
      "|    explained_variance   | 0.491      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 1.67e+05   |\n",
      "|    n_updates            | 1050       |\n",
      "|    policy_gradient_loss | 0.0162     |\n",
      "|    value_loss           | 5.02e+05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 80.3        |\n",
      "|    ep_rew_mean          | 6.46e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 7244        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019102387 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.499      |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.83e+05    |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | 0.0208      |\n",
      "|    value_loss           | 4.61e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 80.1        |\n",
      "|    ep_rew_mean          | 6.62e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 7906        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019378535 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.95e+05    |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | 0.0154      |\n",
      "|    value_loss           | 5.07e+05    |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 88.3      |\n",
      "|    ep_rew_mean          | 7.38e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 11        |\n",
      "|    iterations           | 12        |\n",
      "|    time_elapsed         | 8582      |\n",
      "|    total_timesteps      | 98304     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0195596 |\n",
      "|    clip_fraction        | 0.25      |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | -0.423    |\n",
      "|    explained_variance   | 0.452     |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | 2.67e+05  |\n",
      "|    n_updates            | 1080      |\n",
      "|    policy_gradient_loss | 0.0165    |\n",
      "|    value_loss           | 5.26e+05  |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 83.8        |\n",
      "|    ep_rew_mean          | 7.43e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 9329        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015972372 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.418      |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.02e+05    |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | 0.0163      |\n",
      "|    value_loss           | 4.96e+05    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.set_env(env)\n",
    "model.learn(total_timesteps=100000, callback=callback)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ac1d4-8abc-4c48-ad92-49ee9b49b9c2",
   "metadata": {},
   "source": [
    "**Testing the model `./train/train_Deadly_Corridor_COMP_5_S_5_2/best_model_100000.zip`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63f1542b-f4bc-4dff-8240-670405084c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -1956.1075134277344\n",
      "Episode 2: Total Reward = 9779.831802368164\n",
      "Episode 3: Total Reward = 9828.022354125977\n",
      "Episode 4: Total Reward = -180.59231567382812\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN, PPO \n",
    "import cv2\n",
    "\n",
    "\n",
    "model_path = \"./train/train_Deadly_Corridor_COMP_5_S_5_2/best_model_100000.zip\"  \n",
    "model = PPO.load(model_path)\n",
    "\n",
    "\n",
    "env = VizDoomGym(render=True)  \n",
    "num_episodes = 4\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs,_ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        time.sleep(0.10)\n",
    "        total_reward += reward\n",
    "        done=terminated or truncated\n",
    "        # time.sleep(1)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "    time.sleep(2)\n",
    "  \n",
    "# Close environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c36d2-092e-4209-9432-1632fc6b97c4",
   "metadata": {},
   "source": [
    "**this is the final agent it's very good and it learned how to take cover**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095bc4ef-8cb6-4a74-be24-04c90134b463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
